Basic Units
Words
    basic units in most MT systems
Word segmentation
    word boundaries are not given in all languages
Morphology
    internal structure of words
Part of Speech
    role the word plays within a sentence
    noun, ver, adjective, etc.
Grammatical categories
    property of items within a grammar of language; tense for verb, gender, number for noun

Sentence Structure
Phrase-structure grammar

Semantics
Meaning of natural language constructs

Compositionality
    Meaning of a sentence is composed of meaning of its parts

Lexical Semantics
The meaning of single words
Difficulties:
    one word can have multiple meanings
    Polysemy: word with same surface have different meanings exp. interest, bank
    Homonymy: a word can have totally different meaning exp. can

How to separate each sentence into different words while languages other than English do not use spaces to separate words.

Translation Mismatch
Different information in the source and the target
    Lexical Mismatch
    Difficulty

Translation Divergence
Same information in the source and the target text
Structural Divergence
    word order
Categorical Divergence
    noun -> adjective
Co-refrence:refering to object within and across sentence boundaries
    Anaphora:he, she, it
    Deictic refrence: now, here, I
    Reference iwth synonyms

Language is developing
    new words
    words from other language
    social media

Machine Translation Approaches:
    Rule-based
    Corpus-based

Vauquois Triangle
    Direct
    Transfer
    Interlingua

Direct Approach
    Dictionary-based translation
    Translation memory

Transfer-based Approach
    1. analyze: source sentence=> abstract representation
    2. transfer: source language representation => target language representation
    3. generate: target language representation => surface from of target sentence

Interlingua
    intermediate language
    abstract language-independent representation: pure meaning
    1. analyze
    2. generate 

Statistical Machine Translation
Learn model from data 
Statistical Model:
    create many alternatives
    give score to each alternatives

Neural Machine Translation:
    one model
    end-to-end training 
learn source and target language representation
significant improved translation quality

In the corpus-based approaches, data is the main knowledge source.
Source of data:
    Monolingua in the form of raw text 
    Parallel: pair of sentences in two languages

Notations:
source sentence => f=f1, f2, ..., fI index => i
target sentence => e=e1, e2, ..., ej index => j

Preprocessing:
Idea: Make text as homogenous as possible
Tokenization:
    Separation into words
    European languages mainly splitting of punctuation marks
    Special cases: e.g. abbrevations
True-casing:
    Recase first word in the sentence
Numbers, Named Entites: replace numbers with <NUM>

Evaluation: measuring quality of the translation
Use cases of MT evaluation:
    Application scenario
    guide the research
    system development

Difficulties with the evaluation:
                                            Reference: Everything is different now
    Ambiguity: multiple correct translations => Everything has changed
    small changes can be very important      => Nothing is different now
    subjective                               => some things have changed

Human Evaluation:
    Gold Standard
    Subjective
    Expensive, and time-consuming

Automatic Evaluation:
    Cheap
    Fast
    Human refrence

Granulaity
    Sentence-based
        Error-analysis
        Difficult
    Document-based
        Most commonly used in automatic evalution
    Task-based
        Final goal
        Dependent on several other components 
    
Adequacy vs. Fluency
    Adequacy: Is the menaing translated correctly? => Different is everything now. (adequate but not fluent)
    Fluency: Is the output fluent?                 => Nothing is different now. (fluent but not adequate)

Error Analysis
    Interpretable results
    Identify most prominent error sources
    Error classification
        Missing words, word order, incorrect words
    Test suits

Challenges of Human Evaluation:
    Expensive
    Subjective

Quality Measures:
    Intra annotator agreement
        Agreement between different annotators(Do humans  who evaluate your system agree or disagree)
        Mean k = 0.357 (ranked by different humans)
    Inter annotator agreement
        Agreement within an annotator(shown multiple times to one human to evaluate)
        Mean k = 0.529

Direct Assesment:
    Given:
        Source, Translation
        Optional: reference translation
    Evaluate:
        Performance, adequacy, fluency
    Scale:
        1 - 5
        Percentage
    Problem:
        Normalization across judges

Ranking: 
    Rank different systems according to perfomance
        Ties allowed

Post-editing: to correct the translation
    Measurement effort:    
        Time
        Keystrokes

Task-based Evaluation:
    Ask questions after lecture


BLEU: Bilingual Evaluation Understudy
Translation similar to reference is better
    Count number of equal sentences
    N-gram overlap
        1-gram: how many words are equal in reference and in translation
        2-gram: how many a group of two words are equal in reference and in translation
        Geomatric mean: (1-gram * 2-gram * ... * n-gram) ^ 1 / n 
        Brevity Penalty: 
    Clipping: max count of n-gram in any refrence
    Document level score: aggregate statistics over whole document

Adequency is modeled by word precision
Fluency is modeled by n-gram precision
No recall: brevity penalty to prevent short sentences
        
Word-based Statistical Machine Translation
For implementing these models there is no need hand-written rules, but a large amount of data

Lexicon: Store possible translations of a word
for example to calculate the probability of the german word Wargen, we go through large English corpora,
and then count the number of its different English translations.

ALignment: Mapping between source and target words
exp. NULL Ich besuche einen freund
           I   visit   a    friend

p(e, a | f) = epsilon / (5) ^ 4 * t(I|Ich) * t(visit|besuche) * t(a|einen) * t(friend|freund)
 = epsilon / (5) ^ 4 * 0.5 * 0.3 * 0.8 * 0.4
5 = number of source words in German + NULL

problems if this model:
incomplete data

Expectation-Maximization Algorithm:
1. Initialize the model
2. Expectation step: Apply model to data
3. Maximization step: Learn model from data
4. Iterate step 2, 3 

# Language Model
P(w) is the probability of all the words in a sentence

Break Down
P(w1, w2, .., wl) = P(w1) * P(w2|w1) * P(wl|w1, w2, .., wl-1)

Approximation
P(wl|w1, w2, .., wl-1)= P(wl|C(w1, w2, ..., wl-1))

Markov Assumption
probability only depends on previous n-1 words
P(wl|w1, w2, .., wl-1)= P(wl|wl-n+1, ..., wl-1)


I would like to swim

Unigram P(like)
Bi-gram P(like | would)
Tri-gram P(like| I would)
P(I would like to swim) = P(I | <s> <s>) * P(would| <s> I)
* P(like | I would) * P(to | would like) * P(swim | like to )
* P(</s> | to swim)

P(w3 | w1, w2) = count(w1, w2, w3) / sum of count(w1, w2, w)
P(to | I like) = C(I like to) / sum (I like )

Smoothing
problem:N-gram not seen => P(w) = 0
Solution: 
Interpolation/Back-off
p1(w3|w1w2) = landa2p1(w3)+landa2p2(w3|w2)+landa3p3(w3|w1w2)

